<?xml version='1.0' encoding='UTF-8' standalone='no'?>
<doxygen xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="compound.xsd" version="1.8.17">
  <compounddef id="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil" kind="class" language="Java" prot="public">
    <compoundname>org::apache::iceberg::spark::SparkUtil</compoundname>
      <sectiondef kind="public-static-attrib">
      <memberdef kind="variable" id="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a43cbdd996852420f8f30f5dfefc40050" prot="public" static="yes" mutable="no">
        <type>final String</type>
        <definition>final String org.apache.iceberg.spark.SparkUtil.TIMESTAMP_WITHOUT_TIMEZONE_ERROR</definition>
        <argsstring></argsstring>
        <name>TIMESTAMP_WITHOUT_TIMEZONE_ERROR</name>
        <initializer>=
      String.format(
          &quot;Cannot handle timestamp without&quot;
              + &quot; timezone fields in Spark. Spark does not natively support this type but if you would like to handle all&quot;
              + &quot; timestamps as timestamp with timezone set &apos;%s&apos; to true. This will not change the underlying values stored&quot;
              + &quot; but will change their displayed values in Spark. For more information please see&quot;
              + &quot; https://docs.databricks.com/spark/latest/dataframes-datasets/dates-timestamps.html#ansi-sql-and&quot;
              + &quot;-spark-sql-timestamps&quot;,
          SparkSQLProperties.HANDLE_TIMESTAMP_WITHOUT_TIMEZONE)</initializer>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" line="54" column="30" bodyfile="spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" bodystart="54" bodyend="-1"/>
      </memberdef>
      </sectiondef>
      <sectiondef kind="private-static-attrib">
      <memberdef kind="variable" id="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a80598f720f798f900237ed2d1e6a8484" prot="private" static="yes" mutable="no">
        <type>final String</type>
        <definition>static final String org.apache.iceberg.spark.SparkUtil::SPARK_CATALOG_CONF_PREFIX</definition>
        <argsstring></argsstring>
        <name>SPARK_CATALOG_CONF_PREFIX</name>
        <initializer>= &quot;spark.sql.catalog&quot;</initializer>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" line="64" column="31" bodyfile="spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" bodystart="64" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a1c0eb1dbd593d6849e555e3b1cfa2626" prot="private" static="yes" mutable="no">
        <type>final String</type>
        <definition>static final String org.apache.iceberg.spark.SparkUtil::SPARK_CATALOG_HADOOP_CONF_OVERRIDE_FMT_STR</definition>
        <argsstring></argsstring>
        <name>SPARK_CATALOG_HADOOP_CONF_OVERRIDE_FMT_STR</name>
        <initializer>=
      <ref refid="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a80598f720f798f900237ed2d1e6a8484" kindref="member">SPARK_CATALOG_CONF_PREFIX</ref> + &quot;.%s.hadoop.&quot;</initializer>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" line="69" column="31" bodyfile="spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" bodystart="69" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a34d9b3d57cc8e427f77b8b019038a4f8" prot="private" static="yes" mutable="no">
        <type>final Joiner</type>
        <definition>static final Joiner org.apache.iceberg.spark.SparkUtil::DOT</definition>
        <argsstring></argsstring>
        <name>DOT</name>
        <initializer>= Joiner.on(&quot;.&quot;)</initializer>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" line="72" column="31" bodyfile="spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" bodystart="72" bodyend="-1"/>
      </memberdef>
      </sectiondef>
      <sectiondef kind="private-func">
      <memberdef kind="function" id="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a655e4417f229bef7c180191dc191ac88" prot="private" static="no" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type></type>
        <definition>org.apache.iceberg.spark.SparkUtil.SparkUtil</definition>
        <argsstring>()</argsstring>
        <name>SparkUtil</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" line="74" column="11" bodyfile="spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" bodystart="74" bodyend="74"/>
      </memberdef>
      <memberdef kind="function" id="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a655e4417f229bef7c180191dc191ac88" prot="private" static="no" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type></type>
        <definition>org.apache.iceberg.spark.SparkUtil.SparkUtil</definition>
        <argsstring>()</argsstring>
        <name>SparkUtil</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" line="68" column="11" bodyfile="spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" bodystart="68" bodyend="68"/>
      </memberdef>
      <memberdef kind="function" id="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a655e4417f229bef7c180191dc191ac88" prot="private" static="no" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type></type>
        <definition>org.apache.iceberg.spark.SparkUtil.SparkUtil</definition>
        <argsstring>()</argsstring>
        <name>SparkUtil</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" line="66" column="11" bodyfile="spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" bodystart="66" bodyend="66"/>
      </memberdef>
      </sectiondef>
      <sectiondef kind="public-static-func">
      <memberdef kind="function" id="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1abafc1cdbb263807f6c7f3573d57389c6" prot="public" static="yes" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type>void</type>
        <definition>static void org.apache.iceberg.spark.SparkUtil.validatePartitionTransforms</definition>
        <argsstring>(PartitionSpec spec)</argsstring>
        <name>validatePartitionTransforms</name>
        <param>
          <type><ref refid="classorg_1_1apache_1_1iceberg_1_1PartitionSpec" kindref="compound">PartitionSpec</ref></type>
          <declname>spec</declname>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
<para>Check whether the partition transforms in a spec can be used to write data.</para>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>spec</parametername>
</parameternamelist>
<parameterdescription>
<para>a <ref refid="classorg_1_1apache_1_1iceberg_1_1PartitionSpec" kindref="compound">PartitionSpec</ref> </para>
</parameterdescription>
</parameteritem>
</parameterlist>
<parameterlist kind="exception"><parameteritem>
<parameternamelist>
<parametername>UnsupportedOperationException</parametername>
</parameternamelist>
<parameterdescription>
<para>if the spec contains unknown partition transforms </para>
</parameterdescription>
</parameteritem>
</parameterlist>
</para>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" line="82" column="22" bodyfile="spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" bodystart="82" bodyend="94"/>
      </memberdef>
      <memberdef kind="function" id="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a644396f9fbcf8e6274375f83ff315ad1" prot="public" static="yes" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type>static&lt; C, T &gt; <ref refid="classorg_1_1apache_1_1iceberg_1_1util_1_1Pair" kindref="compound">Pair</ref>&lt; C, T &gt;</type>
        <definition>static &lt;C, T&gt; Pair&lt;C, T&gt; org.apache.iceberg.spark.SparkUtil.catalogAndIdentifier</definition>
        <argsstring>(List&lt; String &gt; nameParts, Function&lt; String, C &gt; catalogProvider, BiFunction&lt; String[], String, T &gt; identiferProvider, C currentCatalog, String[] currentNamespace)</argsstring>
        <name>catalogAndIdentifier</name>
        <param>
          <type>List&lt; String &gt;</type>
          <declname>nameParts</declname>
        </param>
        <param>
          <type>Function&lt; String, C &gt;</type>
          <declname>catalogProvider</declname>
        </param>
        <param>
          <type>BiFunction&lt; String[], String, T &gt;</type>
          <declname>identiferProvider</declname>
        </param>
        <param>
          <type>C</type>
          <declname>currentCatalog</declname>
        </param>
        <param>
          <type>String[]</type>
          <declname>currentNamespace</declname>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
<para>A modified version of Spark&apos;s LookupCatalog.CatalogAndIdentifier.unapply Attempts to find the catalog and identifier a multipart identifier represents</para>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>nameParts</parametername>
</parameternamelist>
<parameterdescription>
<para>Multipart identifier representing a table </para>
</parameterdescription>
</parameteritem>
</parameterlist>
<simplesect kind="return"><para>The CatalogPlugin and Identifier for the table </para>
</simplesect>
</para>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" line="103" column="24" bodyfile="spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" bodystart="103" bodyend="130"/>
      </memberdef>
      <memberdef kind="function" id="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a6f19a717a384ae7affbc0aecffe78181" prot="public" static="yes" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type>boolean</type>
        <definition>static boolean org.apache.iceberg.spark.SparkUtil.hasTimestampWithoutZone</definition>
        <argsstring>(Schema schema)</argsstring>
        <name>hasTimestampWithoutZone</name>
        <param>
          <type><ref refid="classorg_1_1apache_1_1iceberg_1_1Schema" kindref="compound">Schema</ref></type>
          <declname>schema</declname>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
<para>Responsible for checking if the table schema has a timestamp without timezone column</para>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>schema</parametername>
</parameternamelist>
<parameterdescription>
<para>table schema to check if it contains a timestamp without timezone column </para>
</parameterdescription>
</parameteritem>
</parameterlist>
<simplesect kind="return"><para>boolean indicating if the schema passed in has a timestamp field without a timezone </para>
</simplesect>
</para>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" line="138" column="25" bodyfile="spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" bodystart="138" bodyend="140"/>
      </memberdef>
      <memberdef kind="function" id="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1adbcbb0084b587dfb0c3994b15f596d56" prot="public" static="yes" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type>boolean</type>
        <definition>static boolean org.apache.iceberg.spark.SparkUtil.useTimestampWithoutZoneInNewTables</definition>
        <argsstring>(RuntimeConfig sessionConf)</argsstring>
        <name>useTimestampWithoutZoneInNewTables</name>
        <param>
          <type>RuntimeConfig</type>
          <declname>sessionConf</declname>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
<para>Checks whether timestamp types for new tables should be stored with timezone info.</para>
<para>The default value is false and all timestamp fields are stored as {<ref refid="classorg_1_1apache_1_1iceberg_1_1types_1_1Types_1_1TimestampType_1aa9ebac57c38875a6f112d8d809031b21" kindref="member">}. If enabled, all timestamp fields in new tables will be stored as  Types.TimestampType#withoutZone()}.  sessionConf a Spark runtime config  true if timestamp types for new tables should be stored with timezone info </ref></para>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" line="152" column="25" bodyfile="spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" bodystart="152" bodyend="159"/>
      </memberdef>
      <memberdef kind="function" id="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a79eee28d34cca11729596097cbd5767c" prot="public" static="yes" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type>Configuration</type>
        <definition>static Configuration org.apache.iceberg.spark.SparkUtil.hadoopConfCatalogOverrides</definition>
        <argsstring>(SparkSession spark, String catalogName)</argsstring>
        <name>hadoopConfCatalogOverrides</name>
        <param>
          <type>SparkSession</type>
          <declname>spark</declname>
        </param>
        <param>
          <type>String</type>
          <declname>catalogName</declname>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
<para>Pulls any Catalog specific overrides for the Hadoop conf from the current SparkSession, which can be set via <computeroutput>spark.sql.catalog.$catalogName.hadoop.*</computeroutput></para>
<para>Mirrors the override of hadoop configurations for a given spark session using <computeroutput>spark.hadoop.*</computeroutput>.</para>
<para>The <ref refid="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkCatalog" kindref="compound">SparkCatalog</ref> allows for hadoop configurations to be overridden per catalog, by setting them on the SQLConf, where the following will add the property &quot;fs.default.name&quot; with value &quot;hdfs://hanksnamenode:8020&quot; to the catalog&apos;s hadoop configuration. SparkSession.builder() .config(s&quot;spark.sql.catalog.$catalogName.hadoop.fs.default.name&quot;, &quot;hdfs://hanksnamenode:8020&quot;) .getOrCreate()</para>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>spark</parametername>
</parameternamelist>
<parameterdescription>
<para>The current Spark session </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>catalogName</parametername>
</parameternamelist>
<parameterdescription>
<para>Name of the catalog to find overrides for. </para>
</parameterdescription>
</parameteritem>
</parameterlist>
<simplesect kind="return"><para>the Hadoop Configuration that should be used for this catalog, with catalog specific overrides applied. </para>
</simplesect>
</para>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" line="179" column="31" bodyfile="spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" bodystart="179" bodyend="196"/>
      </memberdef>
      <memberdef kind="function" id="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a55a8d38569213c9c8265fd06614e9763" prot="public" static="yes" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type>List&lt; <ref refid="interfaceorg_1_1apache_1_1iceberg_1_1expressions_1_1Expression" kindref="compound">Expression</ref> &gt;</type>
        <definition>static List&lt;Expression&gt; org.apache.iceberg.spark.SparkUtil.partitionMapToExpression</definition>
        <argsstring>(StructType schema, Map&lt; String, String &gt; filters)</argsstring>
        <name>partitionMapToExpression</name>
        <param>
          <type>StructType</type>
          <declname>schema</declname>
        </param>
        <param>
          <type>Map&lt; String, String &gt;</type>
          <declname>filters</declname>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
<para>Get a List of Spark filter Expression.</para>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>schema</parametername>
</parameternamelist>
<parameterdescription>
<para>table schema </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>filters</parametername>
</parameternamelist>
<parameterdescription>
<para>filters in the format of a Map, where key is one of the table column name, and value is the specific value to be filtered on the column. </para>
</parameterdescription>
</parameteritem>
</parameterlist>
<simplesect kind="return"><para>a List of filters in the format of Spark Expression. </para>
</simplesect>
</para>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" line="210" column="22" bodyfile="spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" bodystart="210" bodyend="276"/>
      </memberdef>
      <memberdef kind="function" id="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a2928278de7d94cc958ed04ce1ddc03b3" prot="public" static="yes" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type>String</type>
        <definition>static String org.apache.iceberg.spark.SparkUtil.toColumnName</definition>
        <argsstring>(NamedReference ref)</argsstring>
        <name>toColumnName</name>
        <param>
          <type>NamedReference</type>
          <declname>ref</declname>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" line="278" column="24" bodyfile="spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" bodystart="278" bodyend="280"/>
      </memberdef>
      <memberdef kind="function" id="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1ac4e908474ba82d3ef13eb38bed9e59bb" prot="public" static="yes" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type>boolean</type>
        <definition>static boolean org.apache.iceberg.spark.SparkUtil.caseSensitive</definition>
        <argsstring>(SparkSession spark)</argsstring>
        <name>caseSensitive</name>
        <param>
          <type>SparkSession</type>
          <declname>spark</declname>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" line="282" column="25" bodyfile="spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" bodystart="282" bodyend="284"/>
      </memberdef>
      <memberdef kind="function" id="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1abafc1cdbb263807f6c7f3573d57389c6" prot="public" static="yes" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type>void</type>
        <definition>static void org.apache.iceberg.spark.SparkUtil.validatePartitionTransforms</definition>
        <argsstring>(PartitionSpec spec)</argsstring>
        <name>validatePartitionTransforms</name>
        <param>
          <type><ref refid="classorg_1_1apache_1_1iceberg_1_1PartitionSpec" kindref="compound">PartitionSpec</ref></type>
          <declname>spec</declname>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
<para>Check whether the partition transforms in a spec can be used to write data.</para>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>spec</parametername>
</parameternamelist>
<parameterdescription>
<para>a <ref refid="classorg_1_1apache_1_1iceberg_1_1PartitionSpec" kindref="compound">PartitionSpec</ref> </para>
</parameterdescription>
</parameteritem>
</parameterlist>
<parameterlist kind="exception"><parameteritem>
<parameternamelist>
<parametername>UnsupportedOperationException</parametername>
</parameternamelist>
<parameterdescription>
<para>if the spec contains unknown partition transforms </para>
</parameterdescription>
</parameteritem>
</parameterlist>
</para>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" line="76" column="22" bodyfile="spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" bodystart="76" bodyend="88"/>
      </memberdef>
      <memberdef kind="function" id="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a644396f9fbcf8e6274375f83ff315ad1" prot="public" static="yes" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type>static&lt; C, T &gt; <ref refid="classorg_1_1apache_1_1iceberg_1_1util_1_1Pair" kindref="compound">Pair</ref>&lt; C, T &gt;</type>
        <definition>static &lt;C, T&gt; Pair&lt;C, T&gt; org.apache.iceberg.spark.SparkUtil.catalogAndIdentifier</definition>
        <argsstring>(List&lt; String &gt; nameParts, Function&lt; String, C &gt; catalogProvider, BiFunction&lt; String[], String, T &gt; identiferProvider, C currentCatalog, String[] currentNamespace)</argsstring>
        <name>catalogAndIdentifier</name>
        <param>
          <type>List&lt; String &gt;</type>
          <declname>nameParts</declname>
        </param>
        <param>
          <type>Function&lt; String, C &gt;</type>
          <declname>catalogProvider</declname>
        </param>
        <param>
          <type>BiFunction&lt; String[], String, T &gt;</type>
          <declname>identiferProvider</declname>
        </param>
        <param>
          <type>C</type>
          <declname>currentCatalog</declname>
        </param>
        <param>
          <type>String[]</type>
          <declname>currentNamespace</declname>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
<para>A modified version of Spark&apos;s LookupCatalog.CatalogAndIdentifier.unapply Attempts to find the catalog and identifier a multipart identifier represents</para>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>nameParts</parametername>
</parameternamelist>
<parameterdescription>
<para>Multipart identifier representing a table </para>
</parameterdescription>
</parameteritem>
</parameterlist>
<simplesect kind="return"><para>The CatalogPlugin and Identifier for the table </para>
</simplesect>
</para>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" line="97" column="24" bodyfile="spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" bodystart="97" bodyend="124"/>
      </memberdef>
      <memberdef kind="function" id="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a79eee28d34cca11729596097cbd5767c" prot="public" static="yes" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type>Configuration</type>
        <definition>static Configuration org.apache.iceberg.spark.SparkUtil.hadoopConfCatalogOverrides</definition>
        <argsstring>(SparkSession spark, String catalogName)</argsstring>
        <name>hadoopConfCatalogOverrides</name>
        <param>
          <type>SparkSession</type>
          <declname>spark</declname>
        </param>
        <param>
          <type>String</type>
          <declname>catalogName</declname>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
<para>Pulls any Catalog specific overrides for the Hadoop conf from the current SparkSession, which can be set via <computeroutput>spark.sql.catalog.$catalogName.hadoop.*</computeroutput></para>
<para>Mirrors the override of hadoop configurations for a given spark session using <computeroutput>spark.hadoop.*</computeroutput>.</para>
<para>The <ref refid="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkCatalog" kindref="compound">SparkCatalog</ref> allows for hadoop configurations to be overridden per catalog, by setting them on the SQLConf, where the following will add the property &quot;fs.default.name&quot; with value &quot;hdfs://hanksnamenode:8020&quot; to the catalog&apos;s hadoop configuration. SparkSession.builder() .config(s&quot;spark.sql.catalog.$catalogName.hadoop.fs.default.name&quot;, &quot;hdfs://hanksnamenode:8020&quot;) .getOrCreate()</para>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>spark</parametername>
</parameternamelist>
<parameterdescription>
<para>The current Spark session </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>catalogName</parametername>
</parameternamelist>
<parameterdescription>
<para>Name of the catalog to find overrides for. </para>
</parameterdescription>
</parameteritem>
</parameterlist>
<simplesect kind="return"><para>the Hadoop Configuration that should be used for this catalog, with catalog specific overrides applied. </para>
</simplesect>
</para>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" line="144" column="31" bodyfile="spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" bodystart="144" bodyend="161"/>
      </memberdef>
      <memberdef kind="function" id="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a998fa5a9501d6d49755e4ddf9d0c8cf6" prot="public" static="yes" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type>void</type>
        <definition>static void org.apache.iceberg.spark.SparkUtil.validateTimestampWithoutTimezoneConfig</definition>
        <argsstring>(RuntimeConfig conf)</argsstring>
        <name>validateTimestampWithoutTimezoneConfig</name>
        <param>
          <type>RuntimeConfig</type>
          <declname>conf</declname>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" line="167" column="22" bodyfile="spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" bodystart="167" bodyend="169"/>
      </memberdef>
      <memberdef kind="function" id="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a28df4a1839878bff286f27a3572c593d" prot="public" static="yes" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type>void</type>
        <definition>static void org.apache.iceberg.spark.SparkUtil.validateTimestampWithoutTimezoneConfig</definition>
        <argsstring>(RuntimeConfig conf, Map&lt; String, String &gt; options)</argsstring>
        <name>validateTimestampWithoutTimezoneConfig</name>
        <param>
          <type>RuntimeConfig</type>
          <declname>conf</declname>
        </param>
        <param>
          <type>Map&lt; String, String &gt;</type>
          <declname>options</declname>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
<para>Checks for properties both supplied by Spark&apos;s RuntimeConfig and the read or write options</para>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>conf</parametername>
</parameternamelist>
<parameterdescription>
<para>The RuntimeConfig of the active Spark session </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>options</parametername>
</parameternamelist>
<parameterdescription>
<para>The read or write options supplied when reading/writing a table </para>
</parameterdescription>
</parameteritem>
</parameterlist>
</para>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" line="177" column="22" bodyfile="spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" bodystart="177" bodyend="199"/>
      </memberdef>
      <memberdef kind="function" id="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a55a8d38569213c9c8265fd06614e9763" prot="public" static="yes" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type>List&lt; <ref refid="interfaceorg_1_1apache_1_1iceberg_1_1expressions_1_1Expression" kindref="compound">Expression</ref> &gt;</type>
        <definition>static List&lt;Expression&gt; org.apache.iceberg.spark.SparkUtil.partitionMapToExpression</definition>
        <argsstring>(StructType schema, Map&lt; String, String &gt; filters)</argsstring>
        <name>partitionMapToExpression</name>
        <param>
          <type>StructType</type>
          <declname>schema</declname>
        </param>
        <param>
          <type>Map&lt; String, String &gt;</type>
          <declname>filters</declname>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
<para>Get a List of Spark filter Expression.</para>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>schema</parametername>
</parameternamelist>
<parameterdescription>
<para>table schema </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>filters</parametername>
</parameternamelist>
<parameterdescription>
<para>filters in the format of a Map, where key is one of the table column name, and value is the specific value to be filtered on the column. </para>
</parameterdescription>
</parameteritem>
</parameterlist>
<simplesect kind="return"><para>a List of filters in the format of Spark Expression. </para>
</simplesect>
</para>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" line="209" column="22" bodyfile="spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" bodystart="209" bodyend="275"/>
      </memberdef>
      <memberdef kind="function" id="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a2928278de7d94cc958ed04ce1ddc03b3" prot="public" static="yes" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type>String</type>
        <definition>static String org.apache.iceberg.spark.SparkUtil.toColumnName</definition>
        <argsstring>(NamedReference ref)</argsstring>
        <name>toColumnName</name>
        <param>
          <type>NamedReference</type>
          <declname>ref</declname>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" line="277" column="24" bodyfile="spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" bodystart="277" bodyend="279"/>
      </memberdef>
      <memberdef kind="function" id="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1ac4e908474ba82d3ef13eb38bed9e59bb" prot="public" static="yes" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type>boolean</type>
        <definition>static boolean org.apache.iceberg.spark.SparkUtil.caseSensitive</definition>
        <argsstring>(SparkSession spark)</argsstring>
        <name>caseSensitive</name>
        <param>
          <type>SparkSession</type>
          <declname>spark</declname>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" line="281" column="25" bodyfile="spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" bodystart="281" bodyend="283"/>
      </memberdef>
      <memberdef kind="function" id="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1aa060241c29b23c0484916a8dd5589a0d" prot="public" static="yes" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type>List&lt; String &gt;</type>
        <definition>static List&lt;String&gt; org.apache.iceberg.spark.SparkUtil.executorLocations</definition>
        <argsstring>()</argsstring>
        <name>executorLocations</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" line="285" column="22" bodyfile="spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" bodystart="285" bodyend="292"/>
      </memberdef>
      <memberdef kind="function" id="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1abafc1cdbb263807f6c7f3573d57389c6" prot="public" static="yes" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type>void</type>
        <definition>static void org.apache.iceberg.spark.SparkUtil.validatePartitionTransforms</definition>
        <argsstring>(PartitionSpec spec)</argsstring>
        <name>validatePartitionTransforms</name>
        <param>
          <type><ref refid="classorg_1_1apache_1_1iceberg_1_1PartitionSpec" kindref="compound">PartitionSpec</ref></type>
          <declname>spec</declname>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
<para>Check whether the partition transforms in a spec can be used to write data.</para>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>spec</parametername>
</parameternamelist>
<parameterdescription>
<para>a <ref refid="classorg_1_1apache_1_1iceberg_1_1PartitionSpec" kindref="compound">PartitionSpec</ref> </para>
</parameterdescription>
</parameteritem>
</parameterlist>
<parameterlist kind="exception"><parameteritem>
<parameternamelist>
<parametername>UnsupportedOperationException</parametername>
</parameternamelist>
<parameterdescription>
<para>if the spec contains unknown partition transforms </para>
</parameterdescription>
</parameteritem>
</parameterlist>
</para>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" line="74" column="22" bodyfile="spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" bodystart="74" bodyend="86"/>
      </memberdef>
      <memberdef kind="function" id="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a644396f9fbcf8e6274375f83ff315ad1" prot="public" static="yes" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type>static&lt; C, T &gt; <ref refid="classorg_1_1apache_1_1iceberg_1_1util_1_1Pair" kindref="compound">Pair</ref>&lt; C, T &gt;</type>
        <definition>static &lt;C, T&gt; Pair&lt;C, T&gt; org.apache.iceberg.spark.SparkUtil.catalogAndIdentifier</definition>
        <argsstring>(List&lt; String &gt; nameParts, Function&lt; String, C &gt; catalogProvider, BiFunction&lt; String[], String, T &gt; identiferProvider, C currentCatalog, String[] currentNamespace)</argsstring>
        <name>catalogAndIdentifier</name>
        <param>
          <type>List&lt; String &gt;</type>
          <declname>nameParts</declname>
        </param>
        <param>
          <type>Function&lt; String, C &gt;</type>
          <declname>catalogProvider</declname>
        </param>
        <param>
          <type>BiFunction&lt; String[], String, T &gt;</type>
          <declname>identiferProvider</declname>
        </param>
        <param>
          <type>C</type>
          <declname>currentCatalog</declname>
        </param>
        <param>
          <type>String[]</type>
          <declname>currentNamespace</declname>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
<para>A modified version of Spark&apos;s LookupCatalog.CatalogAndIdentifier.unapply Attempts to find the catalog and identifier a multipart identifier represents</para>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>nameParts</parametername>
</parameternamelist>
<parameterdescription>
<para>Multipart identifier representing a table </para>
</parameterdescription>
</parameteritem>
</parameterlist>
<simplesect kind="return"><para>The CatalogPlugin and Identifier for the table </para>
</simplesect>
</para>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" line="95" column="24" bodyfile="spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" bodystart="95" bodyend="122"/>
      </memberdef>
      <memberdef kind="function" id="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a79eee28d34cca11729596097cbd5767c" prot="public" static="yes" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type>Configuration</type>
        <definition>static Configuration org.apache.iceberg.spark.SparkUtil.hadoopConfCatalogOverrides</definition>
        <argsstring>(SparkSession spark, String catalogName)</argsstring>
        <name>hadoopConfCatalogOverrides</name>
        <param>
          <type>SparkSession</type>
          <declname>spark</declname>
        </param>
        <param>
          <type>String</type>
          <declname>catalogName</declname>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
<para>Pulls any Catalog specific overrides for the Hadoop conf from the current SparkSession, which can be set via <computeroutput>spark.sql.catalog.$catalogName.hadoop.*</computeroutput></para>
<para>Mirrors the override of hadoop configurations for a given spark session using <computeroutput>spark.hadoop.*</computeroutput>.</para>
<para>The <ref refid="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkCatalog" kindref="compound">SparkCatalog</ref> allows for hadoop configurations to be overridden per catalog, by setting them on the SQLConf, where the following will add the property &quot;fs.default.name&quot; with value &quot;hdfs://hanksnamenode:8020&quot; to the catalog&apos;s hadoop configuration. SparkSession.builder() .config(s&quot;spark.sql.catalog.$catalogName.hadoop.fs.default.name&quot;, &quot;hdfs://hanksnamenode:8020&quot;) .getOrCreate()</para>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>spark</parametername>
</parameternamelist>
<parameterdescription>
<para>The current Spark session </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>catalogName</parametername>
</parameternamelist>
<parameterdescription>
<para>Name of the catalog to find overrides for. </para>
</parameterdescription>
</parameteritem>
</parameterlist>
<simplesect kind="return"><para>the Hadoop Configuration that should be used for this catalog, with catalog specific overrides applied. </para>
</simplesect>
</para>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" line="142" column="31" bodyfile="spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" bodystart="142" bodyend="159"/>
      </memberdef>
      <memberdef kind="function" id="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a55a8d38569213c9c8265fd06614e9763" prot="public" static="yes" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type>List&lt; <ref refid="interfaceorg_1_1apache_1_1iceberg_1_1expressions_1_1Expression" kindref="compound">Expression</ref> &gt;</type>
        <definition>static List&lt;Expression&gt; org.apache.iceberg.spark.SparkUtil.partitionMapToExpression</definition>
        <argsstring>(StructType schema, Map&lt; String, String &gt; filters)</argsstring>
        <name>partitionMapToExpression</name>
        <param>
          <type>StructType</type>
          <declname>schema</declname>
        </param>
        <param>
          <type>Map&lt; String, String &gt;</type>
          <declname>filters</declname>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
<para>Get a List of Spark filter Expression.</para>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>schema</parametername>
</parameternamelist>
<parameterdescription>
<para>table schema </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>filters</parametername>
</parameternamelist>
<parameterdescription>
<para>filters in the format of a Map, where key is one of the table column name, and value is the specific value to be filtered on the column. </para>
</parameterdescription>
</parameteritem>
</parameterlist>
<simplesect kind="return"><para>a List of filters in the format of Spark Expression. </para>
</simplesect>
</para>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" line="173" column="22" bodyfile="spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" bodystart="173" bodyend="239"/>
      </memberdef>
      <memberdef kind="function" id="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a2928278de7d94cc958ed04ce1ddc03b3" prot="public" static="yes" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type>String</type>
        <definition>static String org.apache.iceberg.spark.SparkUtil.toColumnName</definition>
        <argsstring>(NamedReference ref)</argsstring>
        <name>toColumnName</name>
        <param>
          <type>NamedReference</type>
          <declname>ref</declname>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" line="241" column="24" bodyfile="spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" bodystart="241" bodyend="243"/>
      </memberdef>
      <memberdef kind="function" id="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1ac4e908474ba82d3ef13eb38bed9e59bb" prot="public" static="yes" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type>boolean</type>
        <definition>static boolean org.apache.iceberg.spark.SparkUtil.caseSensitive</definition>
        <argsstring>(SparkSession spark)</argsstring>
        <name>caseSensitive</name>
        <param>
          <type>SparkSession</type>
          <declname>spark</declname>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" line="245" column="25" bodyfile="spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" bodystart="245" bodyend="247"/>
      </memberdef>
      <memberdef kind="function" id="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1aa060241c29b23c0484916a8dd5589a0d" prot="public" static="yes" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type>List&lt; String &gt;</type>
        <definition>static List&lt;String&gt; org.apache.iceberg.spark.SparkUtil.executorLocations</definition>
        <argsstring>()</argsstring>
        <name>executorLocations</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" line="249" column="22" bodyfile="spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" bodystart="249" bodyend="256"/>
      </memberdef>
      </sectiondef>
      <sectiondef kind="private-static-func">
      <memberdef kind="function" id="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1aa0683d37d29f9b9ce12408fa08eb9bed" prot="private" static="yes" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type>String</type>
        <definition>static String org.apache.iceberg.spark.SparkUtil.hadoopConfPrefixForCatalog</definition>
        <argsstring>(String catalogName)</argsstring>
        <name>hadoopConfPrefixForCatalog</name>
        <param>
          <type>String</type>
          <declname>catalogName</declname>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" line="198" column="25" bodyfile="spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" bodystart="198" bodyend="200"/>
      </memberdef>
      <memberdef kind="function" id="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1aa0683d37d29f9b9ce12408fa08eb9bed" prot="private" static="yes" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type>String</type>
        <definition>static String org.apache.iceberg.spark.SparkUtil.hadoopConfPrefixForCatalog</definition>
        <argsstring>(String catalogName)</argsstring>
        <name>hadoopConfPrefixForCatalog</name>
        <param>
          <type>String</type>
          <declname>catalogName</declname>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" line="163" column="25" bodyfile="spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" bodystart="163" bodyend="165"/>
      </memberdef>
      <memberdef kind="function" id="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a6e04f585cb3b1c8b6390dac0ebdcacc2" prot="private" static="yes" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type>List&lt; BlockManagerId &gt;</type>
        <definition>static List&lt;BlockManagerId&gt; org.apache.iceberg.spark.SparkUtil.fetchPeers</definition>
        <argsstring>(BlockManager blockManager)</argsstring>
        <name>fetchPeers</name>
        <param>
          <type>BlockManager</type>
          <declname>blockManager</declname>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" line="294" column="23" bodyfile="spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" bodystart="294" bodyend="298"/>
      </memberdef>
      <memberdef kind="function" id="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1ae16d40e28d93107bdae44398e6fe0ab6" prot="private" static="yes" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type>static&lt; T &gt; List&lt; T &gt;</type>
        <definition>static &lt;T&gt; List&lt;T&gt; org.apache.iceberg.spark.SparkUtil.toJavaList</definition>
        <argsstring>(Seq&lt; T &gt; seq)</argsstring>
        <name>toJavaList</name>
        <param>
          <type>Seq&lt; T &gt;</type>
          <declname>seq</declname>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" line="300" column="24" bodyfile="spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" bodystart="300" bodyend="302"/>
      </memberdef>
      <memberdef kind="function" id="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a8e3a5f6ca48dcf3f42eec9523d0a3825" prot="private" static="yes" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type>String</type>
        <definition>static String org.apache.iceberg.spark.SparkUtil.toExecutorLocation</definition>
        <argsstring>(BlockManagerId id)</argsstring>
        <name>toExecutorLocation</name>
        <param>
          <type>BlockManagerId</type>
          <declname>id</declname>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" line="304" column="25" bodyfile="spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" bodystart="304" bodyend="306"/>
      </memberdef>
      <memberdef kind="function" id="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1aa0683d37d29f9b9ce12408fa08eb9bed" prot="private" static="yes" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type>String</type>
        <definition>static String org.apache.iceberg.spark.SparkUtil.hadoopConfPrefixForCatalog</definition>
        <argsstring>(String catalogName)</argsstring>
        <name>hadoopConfPrefixForCatalog</name>
        <param>
          <type>String</type>
          <declname>catalogName</declname>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" line="161" column="25" bodyfile="spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" bodystart="161" bodyend="163"/>
      </memberdef>
      <memberdef kind="function" id="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a6e04f585cb3b1c8b6390dac0ebdcacc2" prot="private" static="yes" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type>List&lt; BlockManagerId &gt;</type>
        <definition>static List&lt;BlockManagerId&gt; org.apache.iceberg.spark.SparkUtil.fetchPeers</definition>
        <argsstring>(BlockManager blockManager)</argsstring>
        <name>fetchPeers</name>
        <param>
          <type>BlockManager</type>
          <declname>blockManager</declname>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" line="258" column="23" bodyfile="spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" bodystart="258" bodyend="262"/>
      </memberdef>
      <memberdef kind="function" id="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1ae16d40e28d93107bdae44398e6fe0ab6" prot="private" static="yes" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type>static&lt; T &gt; List&lt; T &gt;</type>
        <definition>static &lt;T&gt; List&lt;T&gt; org.apache.iceberg.spark.SparkUtil.toJavaList</definition>
        <argsstring>(Seq&lt; T &gt; seq)</argsstring>
        <name>toJavaList</name>
        <param>
          <type>Seq&lt; T &gt;</type>
          <declname>seq</declname>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" line="264" column="24" bodyfile="spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" bodystart="264" bodyend="266"/>
      </memberdef>
      <memberdef kind="function" id="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a8e3a5f6ca48dcf3f42eec9523d0a3825" prot="private" static="yes" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type>String</type>
        <definition>static String org.apache.iceberg.spark.SparkUtil.toExecutorLocation</definition>
        <argsstring>(BlockManagerId id)</argsstring>
        <name>toExecutorLocation</name>
        <param>
          <type>BlockManagerId</type>
          <declname>id</declname>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" line="268" column="25" bodyfile="spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" bodystart="268" bodyend="270"/>
      </memberdef>
      </sectiondef>
    <briefdescription>
    </briefdescription>
    <detaileddescription>
    </detaileddescription>
    <collaborationgraph>
      <node id="1">
        <label>org.apache.iceberg.spark.SparkUtil</label>
        <link refid="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil"/>
        <childnode refid="2" relation="usage">
          <edgelabel>DOT</edgelabel>
        </childnode>
      </node>
      <node id="2">
        <label>Joiner</label>
      </node>
    </collaborationgraph>
    <location file="spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" line="52" column="23" bodyfile="spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java" bodystart="52" bodyend="285"/>
    <listofallmembers>
      <member refid="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1ac4e908474ba82d3ef13eb38bed9e59bb" prot="public" virt="non-virtual"><scope>org::apache::iceberg::spark::SparkUtil</scope><name>caseSensitive</name></member>
      <member refid="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1ac4e908474ba82d3ef13eb38bed9e59bb" prot="public" virt="non-virtual"><scope>org::apache::iceberg::spark::SparkUtil</scope><name>caseSensitive</name></member>
      <member refid="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1ac4e908474ba82d3ef13eb38bed9e59bb" prot="public" virt="non-virtual"><scope>org::apache::iceberg::spark::SparkUtil</scope><name>caseSensitive</name></member>
      <member refid="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a644396f9fbcf8e6274375f83ff315ad1" prot="public" virt="non-virtual"><scope>org::apache::iceberg::spark::SparkUtil</scope><name>catalogAndIdentifier</name></member>
      <member refid="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a644396f9fbcf8e6274375f83ff315ad1" prot="public" virt="non-virtual"><scope>org::apache::iceberg::spark::SparkUtil</scope><name>catalogAndIdentifier</name></member>
      <member refid="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a644396f9fbcf8e6274375f83ff315ad1" prot="public" virt="non-virtual"><scope>org::apache::iceberg::spark::SparkUtil</scope><name>catalogAndIdentifier</name></member>
      <member refid="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a34d9b3d57cc8e427f77b8b019038a4f8" prot="private" virt="non-virtual"><scope>org::apache::iceberg::spark::SparkUtil</scope><name>DOT</name></member>
      <member refid="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1aa060241c29b23c0484916a8dd5589a0d" prot="public" virt="non-virtual"><scope>org::apache::iceberg::spark::SparkUtil</scope><name>executorLocations</name></member>
      <member refid="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1aa060241c29b23c0484916a8dd5589a0d" prot="public" virt="non-virtual"><scope>org::apache::iceberg::spark::SparkUtil</scope><name>executorLocations</name></member>
      <member refid="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a6e04f585cb3b1c8b6390dac0ebdcacc2" prot="private" virt="non-virtual"><scope>org::apache::iceberg::spark::SparkUtil</scope><name>fetchPeers</name></member>
      <member refid="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a6e04f585cb3b1c8b6390dac0ebdcacc2" prot="private" virt="non-virtual"><scope>org::apache::iceberg::spark::SparkUtil</scope><name>fetchPeers</name></member>
      <member refid="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a79eee28d34cca11729596097cbd5767c" prot="public" virt="non-virtual"><scope>org::apache::iceberg::spark::SparkUtil</scope><name>hadoopConfCatalogOverrides</name></member>
      <member refid="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a79eee28d34cca11729596097cbd5767c" prot="public" virt="non-virtual"><scope>org::apache::iceberg::spark::SparkUtil</scope><name>hadoopConfCatalogOverrides</name></member>
      <member refid="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a79eee28d34cca11729596097cbd5767c" prot="public" virt="non-virtual"><scope>org::apache::iceberg::spark::SparkUtil</scope><name>hadoopConfCatalogOverrides</name></member>
      <member refid="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1aa0683d37d29f9b9ce12408fa08eb9bed" prot="private" virt="non-virtual"><scope>org::apache::iceberg::spark::SparkUtil</scope><name>hadoopConfPrefixForCatalog</name></member>
      <member refid="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1aa0683d37d29f9b9ce12408fa08eb9bed" prot="private" virt="non-virtual"><scope>org::apache::iceberg::spark::SparkUtil</scope><name>hadoopConfPrefixForCatalog</name></member>
      <member refid="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1aa0683d37d29f9b9ce12408fa08eb9bed" prot="private" virt="non-virtual"><scope>org::apache::iceberg::spark::SparkUtil</scope><name>hadoopConfPrefixForCatalog</name></member>
      <member refid="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a6f19a717a384ae7affbc0aecffe78181" prot="public" virt="non-virtual"><scope>org::apache::iceberg::spark::SparkUtil</scope><name>hasTimestampWithoutZone</name></member>
      <member refid="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a55a8d38569213c9c8265fd06614e9763" prot="public" virt="non-virtual"><scope>org::apache::iceberg::spark::SparkUtil</scope><name>partitionMapToExpression</name></member>
      <member refid="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a55a8d38569213c9c8265fd06614e9763" prot="public" virt="non-virtual"><scope>org::apache::iceberg::spark::SparkUtil</scope><name>partitionMapToExpression</name></member>
      <member refid="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a55a8d38569213c9c8265fd06614e9763" prot="public" virt="non-virtual"><scope>org::apache::iceberg::spark::SparkUtil</scope><name>partitionMapToExpression</name></member>
      <member refid="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a80598f720f798f900237ed2d1e6a8484" prot="private" virt="non-virtual"><scope>org::apache::iceberg::spark::SparkUtil</scope><name>SPARK_CATALOG_CONF_PREFIX</name></member>
      <member refid="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a1c0eb1dbd593d6849e555e3b1cfa2626" prot="private" virt="non-virtual"><scope>org::apache::iceberg::spark::SparkUtil</scope><name>SPARK_CATALOG_HADOOP_CONF_OVERRIDE_FMT_STR</name></member>
      <member refid="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a655e4417f229bef7c180191dc191ac88" prot="private" virt="non-virtual"><scope>org::apache::iceberg::spark::SparkUtil</scope><name>SparkUtil</name></member>
      <member refid="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a655e4417f229bef7c180191dc191ac88" prot="private" virt="non-virtual"><scope>org::apache::iceberg::spark::SparkUtil</scope><name>SparkUtil</name></member>
      <member refid="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a655e4417f229bef7c180191dc191ac88" prot="private" virt="non-virtual"><scope>org::apache::iceberg::spark::SparkUtil</scope><name>SparkUtil</name></member>
      <member refid="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a43cbdd996852420f8f30f5dfefc40050" prot="public" virt="non-virtual"><scope>org::apache::iceberg::spark::SparkUtil</scope><name>TIMESTAMP_WITHOUT_TIMEZONE_ERROR</name></member>
      <member refid="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a2928278de7d94cc958ed04ce1ddc03b3" prot="public" virt="non-virtual"><scope>org::apache::iceberg::spark::SparkUtil</scope><name>toColumnName</name></member>
      <member refid="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a2928278de7d94cc958ed04ce1ddc03b3" prot="public" virt="non-virtual"><scope>org::apache::iceberg::spark::SparkUtil</scope><name>toColumnName</name></member>
      <member refid="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a2928278de7d94cc958ed04ce1ddc03b3" prot="public" virt="non-virtual"><scope>org::apache::iceberg::spark::SparkUtil</scope><name>toColumnName</name></member>
      <member refid="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a8e3a5f6ca48dcf3f42eec9523d0a3825" prot="private" virt="non-virtual"><scope>org::apache::iceberg::spark::SparkUtil</scope><name>toExecutorLocation</name></member>
      <member refid="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a8e3a5f6ca48dcf3f42eec9523d0a3825" prot="private" virt="non-virtual"><scope>org::apache::iceberg::spark::SparkUtil</scope><name>toExecutorLocation</name></member>
      <member refid="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1ae16d40e28d93107bdae44398e6fe0ab6" prot="private" virt="non-virtual"><scope>org::apache::iceberg::spark::SparkUtil</scope><name>toJavaList</name></member>
      <member refid="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1ae16d40e28d93107bdae44398e6fe0ab6" prot="private" virt="non-virtual"><scope>org::apache::iceberg::spark::SparkUtil</scope><name>toJavaList</name></member>
      <member refid="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1adbcbb0084b587dfb0c3994b15f596d56" prot="public" virt="non-virtual"><scope>org::apache::iceberg::spark::SparkUtil</scope><name>useTimestampWithoutZoneInNewTables</name></member>
      <member refid="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1abafc1cdbb263807f6c7f3573d57389c6" prot="public" virt="non-virtual"><scope>org::apache::iceberg::spark::SparkUtil</scope><name>validatePartitionTransforms</name></member>
      <member refid="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1abafc1cdbb263807f6c7f3573d57389c6" prot="public" virt="non-virtual"><scope>org::apache::iceberg::spark::SparkUtil</scope><name>validatePartitionTransforms</name></member>
      <member refid="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1abafc1cdbb263807f6c7f3573d57389c6" prot="public" virt="non-virtual"><scope>org::apache::iceberg::spark::SparkUtil</scope><name>validatePartitionTransforms</name></member>
      <member refid="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a998fa5a9501d6d49755e4ddf9d0c8cf6" prot="public" virt="non-virtual"><scope>org::apache::iceberg::spark::SparkUtil</scope><name>validateTimestampWithoutTimezoneConfig</name></member>
      <member refid="classorg_1_1apache_1_1iceberg_1_1spark_1_1SparkUtil_1a28df4a1839878bff286f27a3572c593d" prot="public" virt="non-virtual"><scope>org::apache::iceberg::spark::SparkUtil</scope><name>validateTimestampWithoutTimezoneConfig</name></member>
    </listofallmembers>
  </compounddef>
</doxygen>
