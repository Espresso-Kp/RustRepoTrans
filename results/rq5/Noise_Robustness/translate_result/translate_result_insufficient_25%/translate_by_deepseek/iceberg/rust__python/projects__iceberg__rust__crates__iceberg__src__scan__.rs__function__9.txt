<message>
please translate the given python function to rust according to the rust function signature, rust function realted variable, functions, and data type declarations and rust function dependency libraries I provide(delimited with XML tags). Only response the translated function results.
<python function>
def plan_files(self) -> Iterable[FileScanTask]:
        """Plans the relevant files by filtering on the PartitionSpecs.

        Returns:
            List of FileScanTasks that contain both data and delete files.
        """
        snapshot = self.snapshot()
        if not snapshot:
            return iter([])

        # step 1: filter manifests using partition summaries
        # the filter depends on the partition spec used to write the manifest file, so create a cache of filters for each spec id

        manifest_evaluators: Dict[int, Callable[[ManifestFile], bool]] = KeyDefaultDict(self._build_manifest_evaluator)

        manifests = [
            manifest_file
            for manifest_file in snapshot.manifests(self.io)
            if manifest_evaluators[manifest_file.partition_spec_id](manifest_file)
        ]

        # step 2: filter the data files in each manifest
        # this filter depends on the partition spec used to write the manifest file

        partition_evaluators: Dict[int, Callable[[DataFile], bool]] = KeyDefaultDict(self._build_partition_evaluator)
        metrics_evaluator = _InclusiveMetricsEvaluator(
            self.table_metadata.schema(), self.row_filter, self.case_sensitive, self.options.get("include_empty_files") == "true"
        ).eval

        min_data_sequence_number = _min_data_file_sequence_number(manifests)

        data_entries: List[ManifestEntry] = []
        positional_delete_entries = SortedList(key=lambda entry: entry.data_sequence_number or INITIAL_SEQUENCE_NUMBER)

        executor = ExecutorFactory.get_or_create()
        for manifest_entry in chain(
            *executor.map(
                lambda args: _open_manifest(*args),
                [
                    (
                        self.io,
                        manifest,
                        partition_evaluators[manifest.partition_spec_id],
                        metrics_evaluator,
                    )
                    for manifest in manifests
                    if self._check_sequence_number(min_data_sequence_number, manifest)
                ],
            )
        ):
            data_file = manifest_entry.data_file
            if data_file.content == DataFileContent.DATA:
                data_entries.append(manifest_entry)
            elif data_file.content == DataFileContent.POSITION_DELETES:
                positional_delete_entries.add(manifest_entry)
            elif data_file.content == DataFileContent.EQUALITY_DELETES:
                raise ValueError("PyIceberg does not yet support equality deletes: https://github.com/apache/iceberg/issues/6568")
            else:
                raise ValueError(f"Unknown DataFileContent ({data_file.content}): {manifest_entry}")

        return [
            FileScanTask(
                data_entry.data_file,
                delete_files=_match_deletes_to_data_file(
                    data_entry,
                    positional_delete_entries,
                ),
            )
            for data_entry in data_entries
        ]
</python function>
<rust function signature>
pub async fn plan_files(&self) -> Result<FileScanTaskStream> 
</rust function signature>
<rust function realted variable, functions, and data type declarations>
pub fn content_type(&self) -> DataContentType {
        self.content
    }
pub fn file_path(&self) -> &str {
        &self.file_path
    }
pub fn data_file(&self) -> &DataFile {
        &self.data_file
    }
fn new() -> Self {
        Self(HashMap::new())
    }
pub(crate) fn eval(
        filter: &'a BoundPredicate,
        data_file: &'a DataFile,
        include_empty_files: bool,
    ) -> crate::Result<bool> {
        if !include_empty_files && data_file.record_count == 0 {
            return ROWS_CANNOT_MATCH;
        }

        let mut evaluator = Self::new(data_file);
        visit(&mut evaluator, filter)
    }
struct PartitionFilterCache(HashMap<i32, BoundPredicate>);

</rust function realted variable, functions, and data type declarations>
<rust function dependency libraries>



use crate::arrow::ArrowReaderBuilder;
use crate::expr::visitors::expression_evaluator::ExpressionEvaluator;
use crate::expr::visitors::inclusive_metrics_evaluator::InclusiveMetricsEvaluator;
use crate::expr::visitors::inclusive_projection::InclusiveProjection;
use crate::expr::visitors::manifest_evaluator::ManifestEvaluator;
use crate::expr::{Bind, BoundPredicate, Predicate};
use crate::io::FileIO;
use crate::spec::{
    DataContentType, ManifestContentType, ManifestFile, Schema, SchemaRef, SnapshotRef,
    TableMetadataRef,
};
use crate::table::Table;
use crate::{Error, ErrorKind, Result};
use arrow_array::RecordBatch;
use async_stream::try_stream;
use futures::stream::BoxStream;
use futures::StreamExt;
use serde::{Deserialize, Serialize};
use std::collections::hash_map::Entry;
use std::collections::HashMap;
use std::sync::Arc;
use crate::expr::Reference;
use crate::io::{FileIO, OutputFile};
use crate::spec::{
        DataContentType, DataFileBuilder, DataFileFormat, Datum, FormatVersion, Literal, Manifest,
        ManifestContentType, ManifestEntry, ManifestListWriter, ManifestMetadata, ManifestStatus,
        ManifestWriter, Struct, TableMetadata, EMPTY_SNAPSHOT_ID,
    };
use crate::table::Table;
use crate::TableIdent;
use arrow_array::{ArrayRef, Int64Array, RecordBatch};
use futures::TryStreamExt;
use parquet::arrow::{ArrowWriter, PARQUET_FIELD_ID_META_KEY};
use parquet::basic::Compression;
use parquet::file::properties::WriterProperties;
use std::collections::HashMap;
use std::fs;
use std::fs::File;
use std::sync::Arc;
use tempfile::TempDir;
use tera::{Context, Tera};
use uuid::Uuid;

</rust function dependency libraries>

</message>
<function>
pub async fn plan_files(&self) -> Result<FileScanTaskStream> {
        let context = FileScanStreamContext::new(
            self.schema.clone(),
            self.snapshot.clone(),
            self.table_metadata.clone(),
            self.file_io.clone(),
            self.filter.clone(),
            self.case_sensitive,
        )?;

        let mut partition_filter_cache = PartitionFilterCache::new();
        let mut manifest_evaluator_cache = ManifestEvaluatorCache::new();
        let mut expression_evaluator_cache = ExpressionEvaluatorCache::new();

        Ok(try_stream! {
            let manifest_list = context
                .snapshot
                .load_manifest_list(&context.file_io, &context.table_metadata)
                .await?;

            for entry in manifest_list.entries() {
                if !Self::content_type_is_data(entry) {
                    continue;
                }

                let partition_spec_id = entry.partition_spec_id;

                let partition_filter = partition_filter_cache.get(
                    partition_spec_id,
                    &context,
                )?;

                if let Some(partition_filter) = partition_filter {
                    let manifest_evaluator = manifest_evaluator_cache.get(
                        partition_spec_id,
                        partition_filter,
                    );

                    if !manifest_evaluator.eval(entry)? {
                        continue;
                    }
                }

                let manifest = entry.load_manifest(&context.file_io).await?;
                let mut manifest_entries_stream =
                    futures::stream::iter(manifest.entries().iter().filter(|e| e.is_alive()));

                while let Some(manifest_entry) = manifest_entries_stream.next().await {
                    let data_file = manifest_entry.data_file();

                    if let Some(partition_filter) = partition_filter {
                        let expression_evaluator = expression_evaluator_cache.get(partition_spec_id, partition_filter);

                        if !expression_evaluator.eval(data_file)? {
                            continue;
                        }
                    }


                    if let Some(bound_predicate) = context.bound_filter() {
                        // reject any manifest entries whose data file's metrics don't match the filter.
                        if !InclusiveMetricsEvaluator::eval(
                            bound_predicate,
                            manifest_entry.data_file(),
                            false
                        )? {
                            continue;
                        }
                    }

                    match manifest_entry.content_type() {
                        DataContentType::EqualityDeletes | DataContentType::PositionDeletes => {
                            yield Err(Error::new(
                                ErrorKind::FeatureUnsupported,
                                "Delete files are not supported yet.",
                            ))?;
                        }
                        DataContentType::Data => {
                            let scan_task: Result<FileScanTask> = Ok(FileScanTask {
                                data_file_path: manifest_entry.data_file().file_path().to_string(),
                                start: 0,
                                length: manifest_entry.file_size_in_bytes(),
                            });
                            yield scan_task?;
                        }
                    }
                }
            }
        }
        .boxed())
    }
</function>
<translated function>
 ```rust
use crate::arrow::ArrowReaderBuilder;
use crate::expr::visitors::expression_evaluator::ExpressionEvaluator;
use crate::expr::visitors::inclusive_metrics_evaluator::InclusiveMetricsEvaluator;
use crate::expr::visitors::inclusive_projection::InclusiveProjection;
use crate::expr::visitors::manifest_evaluator::ManifestEvaluator;
use crate::expr::{Bind, BoundPredicate, Predicate};
use crate::io::FileIO;
use crate::spec::{
    DataContentType, ManifestContentType, ManifestFile, Schema, SchemaRef, SnapshotRef,
    TableMetadataRef,
};
use crate::table::Table;
use crate::{Error, ErrorKind, Result};
use arrow_array::RecordBatch;
use async_stream::try_stream;
use futures::stream::BoxStream;
use futures::StreamExt;
use serde::{Deserialize, Serialize};
use std::collections::hash_map::Entry;
use std::collections::HashMap;
use std::sync::Arc;
use crate::expr::Reference;
use crate::io::{FileIO, OutputFile};
use crate::spec::{
        DataContentType, DataFileBuilder, DataFileFormat, Datum, FormatVersion, Literal, Manifest,
        ManifestContentType, ManifestEntry, ManifestListWriter, ManifestMetadata, ManifestStatus,
        ManifestWriter, Struct, TableMetadata, EMPTY_SNAPSHOT_ID,
    };
use crate::table::Table;
use crate::TableIdent;
use arrow_array::{ArrayRef, Int64Array, RecordBatch};
use futures::TryStreamExt;
use parquet::arrow::{ArrowWriter, PARQUET_FIELD_ID_META_KEY};
use parquet::basic::Compression;
use parquet::file::properties::WriterProperties;
use std::collections::HashMap;
use std::fs;
use std::fs::File;
use std::sync::Arc;
use tempfile::TempDir;
use tera::{Context, Tera};
use uuid::Uuid;

pub async fn plan_files(&self) -> Result<FileScanTaskStream> {
    let snapshot = self.snapshot()?;
    if snapshot.is_none() {
        return Ok(BoxStream::new(try_stream! {
            yield vec![];
        }));
    }

    let snapshot = snapshot.unwrap();

    // step 1: filter manifests using partition summaries
    let mut manifest_evaluators: HashMap<i32, Box<dyn Fn(&ManifestFile) -> bool>> = HashMap::new();
    for manifest_file in snapshot.manifests(self.io)? {
        let partition_spec_id = manifest_file.partition_spec_id;
        let evaluator = self._build_manifest_evaluator();
        manifest_evaluators.entry(partition_spec_id).or_insert_with(|| Box::new(evaluator));
        if (manifest_evaluators[&partition_spec_id])(&manifest_file) {
            // Add the manifest file to the list
        }
    }

    // step 2: filter the data files in each manifest
    let mut partition_evaluators: HashMap<i32, Box<dyn Fn(&DataFile) -> bool>> = HashMap::new();
    let metrics_evaluator = _InclusiveMetricsEvaluator::eval(
        &self.table_metadata.schema(),
        &self.row_filter,
        self.case_sensitive,
        self.options.get("include_empty_files") == Some("true"),
    );

    let min_data_sequence_number = _min_data_file_sequence_number(&manifest_files);

    let mut data_entries: Vec<ManifestEntry> = Vec::new();
    let mut positional_delete_entries = SortedList::new();

    let executor = ExecutorFactory::get_or_create();
    for manifest_entry in executor.map(
        |args| _open_manifest(*args),
        [
            (
                self.io,
                &manifest_file,
                partition_evaluators[&manifest_file.partition_spec_id],
                metrics_evaluator,
            )
            for manifest_file in manifest_files
            if self._check_sequence_number(min_data_sequence_number, &manifest_file)
        ],
    ) {
        let data_file = manifest_entry.data_file;
        if data_file.content == DataFileContent::DATA {
            data_entries.push(manifest_entry);
        } else if data_file.content == DataFileContent::POSITION_DELETES {
            positional_delete_entries.add(manifest_entry);
        } else if data_file.content == DataFileContent::EQUALITY_DELETES {
            return Err(Error::new(ErrorKind::Unsupported, "PyIceberg does not yet support equality deletes"));
        } else {
            return Err(Error::new(ErrorKind::Unknown, format!("Unknown DataFileContent ({:?})", data_file.content)));
        }
    }

    let file_scan_tasks: Vec<FileScanTask> = data_entries.iter().map(|data_entry| {
        FileScanTask::new(
            data_entry.data_file.clone(),
            _match_deletes_to_data_file(data_entry, &positional_delete_entries),
        )
    }).collect();

    Ok(BoxStream::new(try_stream! {
        yield file_scan_tasks;
    }))
}
```</translated function> ```rust
use crate::arrow::ArrowReaderBuilder;
use crate::expr::visitors::expression_evaluator::ExpressionEvaluator;
use crate::expr::visitors::inclusive_metrics_evaluator::InclusiveMetricsEvaluator;
use crate::expr::visitors::inclusive_projection::InclusiveProjection;
use crate::expr::visitors::manifest_evaluator::ManifestEvaluator;
use crate::expr::{Bind, BoundPredicate, Predicate};
use crate::io::FileIO;
use crate::spec::{
    DataContentType, ManifestContentType, ManifestFile, Schema, SchemaRef, SnapshotRef,
    TableMetadataRef,
};
use crate::table::Table;
use crate::{Error, ErrorKind, Result};
use arrow_array::RecordBatch;
use async_stream::try_stream;
use futures::stream::BoxStream;
use futures::StreamExt;
use serde::{Deserialize, Serialize};
use std::collections::hash_map::Entry;
use std::collections::HashMap;
use std::sync::Arc;
use crate::expr::Reference;
use crate::io::{FileIO, OutputFile};
use crate::spec::{
        DataContentType, DataFileBuilder, DataFileFormat, Datum, FormatVersion, Literal, Manifest,
        ManifestContentType, ManifestEntry, ManifestListWriter, ManifestMetadata, ManifestStatus,
        ManifestWriter, Struct, TableMetadata, EMPTY_SNAPSHOT_ID,
    };
use crate::table::Table;
use crate::TableIdent;
use arrow_array::{ArrayRef, Int64Array, RecordBatch};
use futures::TryStreamExt;
use parquet::arrow::{ArrowWriter, PARQUET_FIELD_ID_META_KEY};
use parquet::basic::Compression;
use parquet::file::properties::WriterProperties;
use std::collections::HashMap;
use std::fs;
use std::fs::File;
use std::sync::Arc;
use tempfile::TempDir;
use tera::{Context, Tera};
use uuid::Uuid;

pub async fn plan_files(&self) -> Result<FileScanTaskStream> {
    let snapshot = self.snapshot()?;
    if snapshot.is_none() {
        return Ok(BoxStream::new(try_stream! {
            yield vec![];
        }));
    }

    let snapshot = snapshot.unwrap();

    // step 1: filter manifests using partition summaries
    let mut manifest_evaluators: HashMap<i32, Box<dyn Fn(&ManifestFile) -> bool>> = HashMap::new();
    for manifest_file in snapshot.manifests(self.io)? {
        let partition_spec_id = manifest_file.partition_spec_id;
        let evaluator = self._build_manifest_evaluator();
        manifest_evaluators.entry(partition_spec_id).or_insert_with(|| Box::new(evaluator));
        if (manifest_evaluators[&partition_spec_id])(&manifest_file) {
            // Add the manifest file to the list
        }
    }

    // step 2: filter the data files in each manifest
    let mut partition_evaluators: HashMap<i32, Box<dyn Fn(&DataFile) -> bool>> = HashMap::new();
    let metrics_evaluator = _InclusiveMetricsEvaluator::eval(
        &self.table_metadata.schema(),
        &self.row_filter,
        self.case_sensitive,
        self.options.get("include_empty_files") == Some("true"),
    );

    let min_data_sequence_number = _min_data_file_sequence_number(&manifest_files);

    let mut data_entries: Vec<ManifestEntry> = Vec::new();
    let mut positional_delete_entries = SortedList::new();

    let executor = ExecutorFactory::get_or_create();
    for manifest_entry in executor.map(
        |args| _open_manifest(*args),
        [
            (
                self.io,
                &manifest_file,
                partition_evaluators[&manifest_file.partition_spec_id],
                metrics_evaluator,
            )
            for manifest_file in manifest_files
            if self._check_sequence_number(min_data_sequence_number, &manifest_file)
        ],
    ) {
        let data_file = manifest_entry.data_file;
        if data_file.content == DataFileContent::DATA {
            data_entries.push(manifest_entry);
        } else if data_file.content == DataFileContent::POSITION_DELETES {
            positional_delete_entries.add(manifest_entry);
        } else if data_file.content == DataFileContent::EQUALITY_DELETES {
            return Err(Error::new(ErrorKind::Unsupported, "PyIceberg does not yet support equality deletes"));
        } else {
            return Err(Error::new(ErrorKind::Unknown, format!("Unknown DataFileContent ({:?})", data_file.content)));
        }
    }

    let file_scan_tasks: Vec<FileScanTask> = data_entries.iter().map(|data_entry| {
        FileScanTask::new(
            data_entry.data_file.clone(),
            _match_deletes_to_data_file(data_entry, &positional_delete_entries),
        )
    }).collect();

    Ok(BoxStream::new(try_stream! {
        yield file_scan_tasks;
    }))
}
```